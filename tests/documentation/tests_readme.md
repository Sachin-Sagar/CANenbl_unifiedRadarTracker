# Tests Documentation

This document provides an overview of the test suite, its structure, and instructions on how to run the tests.

## Folder Structure

The `tests` directory is organized as follows:

```
tests/
├── tests_main.py           # Main interactive script to discover and run tests
├── lib/                    # Test runner scripts and utilities
│   ├── can_service_test_runner.py
│   ├── data_corruption_test_runner.py
│   ├── dual_pipeline_test_runner.py
│   └── live_data_pipeline_test_runner.py
│   └── main_app_logic_test_runner.py
├── test_cases/             # Contains individual test files (unittest.TestCase)
│   ├── test_can_log_playback.py
│   ├── test_can_service.py
│   ├── test_data_corruption.py
│   ├── test_dual_pipeline_simulation.py
│   ├── test_live_data_pipeline.py
│   └── test_main_app_logic.py
├── output/                 # Contains output from test runs
│   └── test_run_YYYYMMDD_HHMMSS/ # Timestamped folder for each run
│       ├── tst_console_out.txt   # Console output for the run
│       └── ...                   # Other test-specific output files
├── test_data/              # Data files used by tests
└── documentation/          # Test-related documentation
    ├── tests.md            # Detailed documentation of test architecture and fixes
    └── tests_readme.md     # This file
```

## How to Run Tests

The primary way to run tests is through the interactive test runner, `tests_main.py`.

### Interactive Test Runner

Execute the `tests_main.py` script from the project root directory:

```bash
python3 tests/tests_main.py
```

This script will discover all available tests and present you with a menu:

```
======================================================================
                         AVAILABLE TESTS
======================================================================
  [1] TestCANLogPlayback.test_can_log_playback
  [2] TestCANService.test_can_service_integration
  ...
  [10] TestMainAppLogic.test_dual_pipeline_from_log_file

  [all] Run all tests
======================================================================

Enter test number(s) to run (e.g., '1,3,5'), or 'all':
```

You can choose to:
-   Run a single test by entering its number (e.g., `1`).
-   Run multiple tests by entering a comma-separated list of numbers (e.g., `1,3,5`).
-   Run all tests by entering `all`.

### Test Output

All output from a test run is saved to a unique, timestamped directory within `tests/output/`. This includes:
-   `tst_console_out.txt`: The complete console output for the test run.
-   Any log files generated by specific tests (e.g., `worker_1_high_freq.log`).

At the end of the run, a summary of passed, failed, and errored tests is printed to the console.

## Key Test Cases

### `test_data_corruption.py`
-   **Purpose**: Verifies the robustness of the shared data dictionary used in the live pipeline.
-   **Tests**:
    -   `test_race_condition_with_lock`: Confirms that using a `multiprocessing.Lock` prevents data loss during concurrent writes.
    -   `test_race_condition_without_lock`: Demonstrates that data loss is possible (though not guaranteed on every run) without a lock. This test is expected to pass even if the race condition doesn't occur in a specific run.

### `test_live_data_pipeline.py`
-   **Purpose**: A suite of integration tests for the live data pipeline, where a mock CAN logger provides data to a mock radar tracker.
-   **Tests**:
    -   `test_data_integrity_in_live_pipeline`: Checks that data flows correctly from the mock CAN logger to the radar tracker.
    -   `test_startup_race_condition_fix`: Ensures the radar tracker waits for the CAN logger to be ready before processing data.
    -   `test_imu_stuck_flag_ignores_grade`: Verifies that the tracker correctly ignores road grade when the IMU "stuck" flag is active.

### `test_dual_pipeline_simulation.py`
-   **Purpose**: Simulates the entire dual-pipeline CAN logger architecture using a pre-recorded log file to debug the core processing logic without hardware.
-   **Functionality**: Dispatches CAN messages to high-frequency and low-frequency worker pools and verifies that signals are processed correctly. It generates detailed logs for each worker pipeline.

### `test_main_app_logic.py`
-   **Purpose**: A hardware-free integration test of the main application's dual-pipeline architecture, using the actual application code from `src/can_logger_app`.
-   **Note**: This test is now **passing**. The previous failures were resolved by correcting the test's data file paths and implementing a robust polling mechanism to handle the asynchronous nature of the multiprocessing workers.
