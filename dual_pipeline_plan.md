# Plan: Implement the "Dual Pipeline" CAN Processing Architecture

**Objective:** Refactor the `can_logger_app` to process high-frequency (10ms) and low-frequency (100ms) signals in two separate, parallel pipelines. This will ensure that a burst of low-frequency messages cannot delay the processing of time-sensitive, high-frequency data.

---

### **File-by-File Change Plan**

#### 1. `src/can_logger_app/main.py` — The Pipeline Orchestrator

This file is the central hub where all the processes and data queues are created and managed. It will require the most significant changes to construct and coordinate the two pipelines.

*   **Why It Must Change:** It currently creates a single, shared pipeline for all signals. This needs to be replaced with a dual-pipeline structure.

*   **Planned Changes:**
    1.  **Create Two Input Queues:** Instead of one `raw_mp_queue`, I will create two distinct `multiprocessing.Queue` objects:
        *   `high_freq_raw_queue`: To hold raw messages for the 10ms pipeline.
        *   `low_freq_raw_queue`: To hold raw messages for the 100ms pipeline.
    2.  **Create Two Worker Pools:** The single pool of decoding workers will be replaced by two specialized pools:
        *   **High-Frequency Pool:** A group of `multiprocessing.Process` workers (e.g., 2 or 3 processes) will be created. They will be configured to read *only* from the `high_freq_raw_queue`.
        *   **Low-Frequency Pool:** A second, smaller group (e.g., 1 process) will be created to read *only* from the `low_freq_raw_queue`.
    3.  **Update Worker Initialization:** The `for` loop that starts the `processing_worker` processes will be split into two separate loops. Each loop will instantiate and start the workers for its respective pool, passing them the correct input queue (`high_freq_raw_queue` or `low_freq_raw_queue`).
    4.  **Update Shutdown Logic:** The graceful shutdown sequence in the `finally` block must be updated to manage both worker pools. This involves sending the `None` stop signal to both `high_freq_raw_queue` and `low_freq_raw_queue` and then joining all processes from both pools.

---

#### 2. `src/can_logger_app/can_handler.py` — The Message Dispatcher

The `CANReader` thread in this file reads messages directly from the hardware. Its role will be expanded to act as an intelligent dispatcher.

*   **Why It Must Change:** It currently places all incoming messages into a single queue. It needs to be modified to sort messages into the new high-priority and low-priority queues.

*   **Planned Changes:**
    1.  **Update `CANReader` Constructor (`__init__`):** The constructor currently accepts a dictionary of queues. It will be changed to accept two distinct queue arguments: `high_freq_queue` and `low_freq_queue`.
    2.  **Update `CANReader` Logic (`run` method):** The core message-handling loop will be modified. For each message received from the CAN bus, it will:
        *   Look up the message's arbitration ID in the `id_to_queue_map` (which is already generated by `utils.py`).
        *   Based on the result ('high' or 'low'), it will `put` the raw message object into the corresponding queue (`self.high_freq_queue` or `self.low_freq_queue`).

---

#### 3. `src/can_logger_app/data_processor.py` — The Decoding Worker

This file contains the `processing_worker` function, which performs the CPU-intensive decoding.

*   **Why It Must Change:** No changes are anticipated.
*   **Planned Changes:**
    *   The function is already designed generically. It accepts a `raw_queue` to read from and a `results_queue` to write to. Because of this decoupled design, we don't need to change the worker's code. We can simply launch the same worker function in our two different pools, and it will correctly process data from whichever input queue it's given. This is a key advantage of the existing design that we will leverage.

---

#### 4. `src/can_logger_app/utils.py` — The Configuration Helper

This file reads the `master_sigList.txt` and prepares the necessary data structures.

*   **Why It Must Change:** No changes are needed.
*   **Planned Changes:**
    *   The function `load_signals_to_monitor` already produces the `id_to_queue_map` that categorizes each CAN message ID as 'high' or 'low' frequency. This existing output is exactly what the new dispatcher in `can_handler.py` will use to route messages. No modifications are necessary.

---
### **Summary of Plan**

| File to Change                               | Reason for Change                                                              | 
| :------------------------------------------- | :----------------------------------------------------------------------------- |
| **`src/can_logger_app/main.py`**             | **Orchestration:** To build and manage the two separate processing pipelines.      |
| **`src/can_logger_app/can_handler.py`**      | **Dispatching:** To sort incoming CAN messages into the correct high/low frequency queue. |
| `src/can_logger_app/data_processor.py`       | **(No Change)** The generic worker function is reusable for both pipelines.      |
| `src/can_logger_app/utils.py`                | **(No Change)** The existing logic already provides the necessary message categorization. |
